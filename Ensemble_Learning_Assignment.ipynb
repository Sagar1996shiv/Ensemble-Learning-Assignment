{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning\n",
        "\n",
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        " - Ensemble learning in machine learning is a technique where multiple models (often called “weak learners”) are trained and then combined to make predictions that are usually more accurate and robust than the predictions of any single model.\n",
        "\n",
        " The central concept is:\n",
        "\n",
        "\"Many weak learners can combine to form a strong learner.\"\n",
        "\n",
        "Instead of relying on one model (which might have high bias, high variance, or miss certain patterns), ensemble methods aggregate the outputs of several models to reduce errors and improve generalization.\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        " - Feature\t      Bagging (Bootstrap Aggregating)\t          Boosting\n",
        "\n",
        "Main Goal\t          Reduce variance\t                  Reduce bias (and variance)\n",
        "\n",
        "Training Approach\t   Models are trained in parallel on different random subsets of data.\t                                         \n",
        "                                                    Models are trained sequentially, each new model focuses on errors of the previous one.\n",
        "\n",
        "Data Sampling\t     Uses bootstrap sampling (random sampling with replacement).\t                                    \n",
        "                                                   Each new model gets reweighted data, giving more weight to misclassified samples.\n",
        "\n",
        "Model Independence\t   All models are independent of each other.\n",
        "                                                   Each model depends on the previous model's results.\n",
        "\n",
        "Error Handling\t       Averages results (classification → majority vote, regression → mean).\n",
        "                                                   Combines models with weighted voting/weighted sum, emphasizing stronger learners.\n",
        "\n",
        "Overfitting Tendency\t  Good at preventing overfitting (especially with unstable learners like decision trees).\n",
        "                                                   More prone to overfitting if not tuned (but can achieve very low training error).\n",
        "Common Algorithms\t      Random Forest\n",
        "                                                   AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        " - Bootstrap sampling is a random sampling technique with replacement used to create multiple new datasets (called bootstrap samples) from the original training data.\n",
        "\n",
        " Role in Bagging (e.g., Random Forest)\n",
        "\n",
        "In Bagging methods like Random Forest:\n",
        "\n",
        "Each model (e.g., decision tree) is trained on a different bootstrap sample.\n",
        "\n",
        "Since each model sees a slightly different dataset, they make different errors.\n",
        "\n",
        "Predictions from all models are then aggregated (majority vote for classification, average for regression).  \n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        " - Out-of-Bag (OOB) samples are the training samples not selected in a given bootstrap sample when building an ensemble model like a Random Forest.\n",
        "\n",
        "OOB Score: How It’s Used\n",
        "The OOB score is a built-in way to evaluate the performance of bagging models without needing a separate validation set.\n",
        "\n",
        "Process:\n",
        "\n",
        "For each training sample:\n",
        "\n",
        "It is an OOB sample for some subset of the models (trees).\n",
        "\n",
        "To make a prediction for that sample:\n",
        "\n",
        "Only use the models for which it was OOB.\n",
        "\n",
        "Compare the aggregated OOB predictions to the true label.\n",
        "\n",
        "Compute the OOB accuracy (classification) or OOB error (regression).\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        " - Aspect\t             Single Decision Tree\t             Random Forest\n",
        "\n",
        "How importance\n",
        "is calculated\t        \n",
        "                       Based on how much each feature reduces impurity (e.g.,\n",
        "Gini index or entropy) across all the splits where it’s used.\n",
        "                                                       Same method (impurity reduction), but averaged across all trees in the forest.\n",
        "\n",
        "Stability of\n",
        "importance\t           \n",
        "                       Can be unstable — small changes in data can lead to very different trees and rankings.\n",
        "                                                       More stable — averaging over many trees smooths out variability.\n",
        "\n",
        "Bias\t                \n",
        "                       Can be biased toward features with many categories or continuous variables.\n",
        "                                                       Still has some bias, but reduced due to aggregation over many random feature subsets.\n",
        "\n",
        "Interpretability\n",
        "                       Easier to interpret — the tree structure shows exactly how the feature was used in splits.\n",
        "                                                       Harder to directly visualize — importance is a statistical summary, not a single path.\n",
        "\n",
        "Overfitting risk\n",
        "                       Higher — importance may reflect noise if the tree overfits.\n",
        "                                                       Lower — averaging reduces the effect of noise.\n",
        "\n",
        "Usefulness\n",
        "                      Good for quick, interpretable insights on small datasets.\n",
        "                      \n",
        "                                                       Better for robust and generalizable importance estimates in large or noisy datasets.\n",
        "\n",
        "6. Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        " - Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n"
      ],
      "metadata": {
        "id": "Y8PGBgLo7M-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AyTL5ph7K3b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nTarget Names:\", data.target_names)\n",
        "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Optional: Display feature names\n",
        "print(\"\\nFeature Names:\")\n",
        "print(data.feature_names)\n"
      ],
      "metadata": {
        "id": "V2duSocwFydw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Train a Random Forest Classifier"
      ],
      "metadata": {
        "id": "bLb0dU3EF2NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,       # Number of trees\n",
        "    random_state=42,        # For reproducibility\n",
        "    oob_score=True          # Enable Out-of-Bag score\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# OOB Score\n",
        "print(\"OOB Score:\", rf_model.oob_score_)\n",
        "\n",
        "# Feature Importance\n",
        "import pandas as pd\n",
        "feature_importances = pd.Series(rf_model.feature_importances_, index=data.feature_names)\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "print(feature_importances.sort_values(ascending=False).head())\n"
      ],
      "metadata": {
        "id": "1LM8sm9cF6S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "LCRScKSDGQVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Create a Pandas Series for feature importance\n",
        "feature_importances = pd.Series(\n",
        "    rf_model.feature_importances_,\n",
        "    index=data.feature_names\n",
        ")\n",
        "\n",
        "# Print top 5 features\n",
        "top_5_features = feature_importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "id": "g8_GRxCGGSU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        " - Train a Bagging Classifier using Decision Trees on the Iris dataset"
      ],
      "metadata": {
        "id": "5kKvCreBw1CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Create a Decision Tree classifier\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Create a Bagging Classifier using Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_estimator,      # Base model\n",
        "    n_estimators=50,               # Number of trees\n",
        "    max_samples=0.8,               # % of samples per tree\n",
        "    max_features=1.0,              # % of features per tree\n",
        "    bootstrap=True,                # Sampling with replacement\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5. Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# 7. Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "ZTgRydnSxLUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "YZrtjgmaxNo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a single Decision Tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "y_pred_tree = single_tree.predict(X_test)\n",
        "tree_accuracy = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "# 4. Train a Bagging Classifier using Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# 5. Print results\n",
        "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {bagging_accuracy:.2f}\")\n",
        "\n",
        "# 6. Compare and interpret\n",
        "if bagging_accuracy > tree_accuracy:\n",
        "    print(\"✅ Bagging outperformed the single Decision Tree.\")\n",
        "elif bagging_accuracy < tree_accuracy:\n",
        "    print(\"⚠️ Single Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"🔹 Both models performed equally well.\")\n"
      ],
      "metadata": {
        "id": "1keBuH8vxcIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        " - Train a Random Forest Classifier"
      ],
      "metadata": {
        "id": "60vOLfQHxfKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Create the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,     # Number of trees\n",
        "    max_depth=None,       # No depth limit\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train the Random Forest Classifier\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# 6. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "guPPq0oWxrcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UW0SvdMlyJ5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Tune hyperparameters max_depth and n_estimators using GridSearchCV"
      ],
      "metadata": {
        "id": "V7qwR2Bcx3LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# 5. Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 6. Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Best parameters and best score from grid search\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# 8. Evaluate on test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy with Best Parameters: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "WB8C9pF_yP-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "LpsC1RjoyTY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Random Forest model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# 5. Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Final accuracy on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "mMBRZ8AVycLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        " - Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset"
      ],
      "metadata": {
        "id": "K9G_wCMkyfYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "\n",
        "# 4. Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# 5. Evaluation function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{model_name} -> RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
        "\n",
        "# 6. Compare results\n",
        "evaluate_model(y_test, y_pred_bagging, \"Bagging Regressor\")\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")\n"
      ],
      "metadata": {
        "id": "XJp_u6SVymSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "DZu7fZoYy089"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "\n",
        "# 4. Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Errors\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# 6. Print comparison\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "if mse_bagging < mse_rf:\n",
        "    print(\"✅ Bagging Regressor performed better (lower MSE).\")\n",
        "elif mse_bagging > mse_rf:\n",
        "    print(\"✅ Random Forest Regressor performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"🔹 Both models performed equally (same MSE).\")\n"
      ],
      "metadata": {
        "id": "HHmG8Kmyy2i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        " - Choose between Bagging or Boosting\n",
        "\n",
        " Step-by-step approach\n",
        "\n",
        "- Define business objective & constraints\n",
        "\n",
        "Target metric(s): AUC-ROC, AU-PR, precision@k (top N risky), recall at fixed false positive rate, expected monetary loss (cost-sensitive metric).\n",
        "\n",
        "Constraints: latency for scoring, interpretability requirements, regulatory explainability, model maintenance budget, training/inference cost.\n",
        "\n",
        "- Data audit & leakage prevention\n",
        "\n",
        "Explore label balance, missingness, feature distributions, outliers, and timestamp fields.\n",
        "\n",
        "Prevent leakage: use time-based splits (train on earlier periods, validate on later periods) because transaction history is temporal.\n",
        "\n",
        "- Select evaluation strategy\n",
        "\n",
        "Use time-series cross-validation (rolling/windows) or holdout by date. Don’t use random CV if time dependence exists.\n",
        "\n",
        "Report multiple metrics: AUROC, AUPR, precision@k, recall@k, and expected loss using your cost matrix.\n",
        "\n",
        "- Baseline sanity checks\n",
        "\n",
        "Train a simple logistic regression and a single decision tree. If they already achieve most of the signal, complexity may not be needed.\n",
        "\n",
        "- Feature engineering & preprocessing\n",
        "\n",
        "Aggregate transaction features over meaningful windows (30/90/180 days), compute delinquencies, balances, velocity metrics, ratios.\n",
        "\n",
        "Encode categoricals (one-hot/target encoding) or use CatBoost (handles categories natively).\n",
        "\n",
        "Ensure feature pipelines are reproducible (scaling, imputation, encoders).\n",
        "\n",
        "- Handle class imbalance\n",
        "\n",
        "Try class weights, focal loss (for boosting), or careful sampling (but avoid synthetic sampling that breaks time-order).\n",
        "\n",
        "Also evaluate precision@k — business often cares about the top flagged loans.\n",
        "\n",
        "- Controlled model comparison experiment\n",
        "\n",
        "Use the same preprocessed data and time-aware CV for all models.\n",
        "\n",
        "Models to compare:\n",
        "\n",
        "Bagging: RandomForest (tune n_estimators, max_depth, max_features).\n",
        "\n",
        "Boosting: LightGBM / XGBoost / CatBoost (tune n_estimators/learning_rate, max_depth, num_leaves).\n",
        "\n",
        "Use early stopping for boosting on validation folds to avoid overfitting.\n",
        "\n",
        "Use the same scoring function (e.g., AUROC or expected loss).\n",
        "\n",
        "- Hyperparameter tuning\n",
        "\n",
        "Use randomized search or Bayesian optimization (Optuna) over reasonable ranges.\n",
        "\n",
        "Keep tuning budgets comparable for a fair trade-off on hyperparameter exploration.\n",
        "\n",
        "- Compare by business metric & stability\n",
        "\n",
        "Primary decision criterion: improvement in business metric (expected loss, precision@k, etc.), not just AUROC.\n",
        "\n",
        "Check stability across time slices: does model performance degrade/variance increase on newer periods?\n",
        "\n",
        "Check calibration — predicted probabilities must be usable for risk scoring or expected loss calculation (use calibration or isotonic regression if needed).\n",
        "\n",
        "- Interpretability & governance checks\n",
        "\n",
        "Run SHAP/feature importance, partial dependence for both models.\n",
        "\n",
        "If boosting is a black box for stakeholders/regulators, consider simpler surrogates or restricted models (monotonic constraints, rule extraction).\n",
        "\n",
        "- Robustness, fairness & post-hoc checks\n",
        "\n",
        "Test for bias across protected groups, adversarial/feature drift scenarios.\n",
        "\n",
        "Simulate cost impact (financial backtest): how many defaults avoided vs. false positives?\n",
        "\n",
        "- Production & monitoring considerations\n",
        "\n",
        "Inference latency, model size, retraining frequency, feature availability at scoring time.\n",
        "\n",
        "Monitoring: data drift, performance drift, feature distribution shift.\n",
        "\n",
        "- Decide — and consider hybrid options\n",
        "\n",
        "If boosting consistently improves the business metric and passes governance checks → choose boosting.\n",
        "\n",
        "If label noise is high, you need very stable feature importances, or model simplicity is required → choose bagging.\n",
        "\n",
        "If both have complementary strengths, consider stacking (meta-learner) or ensembling RF + GBM and re-evaluate business metric.\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "Step-by-step approach\n",
        "\n",
        " - Set the right success criterion\n",
        "\n",
        "Pick business-relevant metrics (e.g., precision@K, recall@K, AUPR, expected monetary loss) in addition to AUROC. Overfitting judged by the wrong metric can mislead you.\n",
        "\n",
        " - Use time-aware validation (no leakage)\n",
        "\n",
        "Split data by time: train on earlier dates, validate on later dates (rolling / walk-forward CV). Never mix future transactions into training folds.\n",
        "\n",
        "Report metrics on a held-out temporal test set (final backtest period).\n",
        "\n",
        " - Start with simple baselines\n",
        "\n",
        "Train logistic regression and a shallow decision tree. Large gaps between train and validation for these indicate systemic issues (data leakage / label noise) before you even try ensembles.\n",
        "\n",
        " - Limit model complexity\n",
        "\n",
        "For tree ensembles, hyperparameters that control complexity are first-line defenses:\n",
        "\n",
        "max_depth, min_samples_leaf (or min_child_weight), max_features.\n",
        "\n",
        "For boosting: use learning_rate (shrinkage), smaller num_leaves / max_depth, and n_estimators with early stopping.\n",
        "\n",
        " - Use regularization\n",
        "\n",
        "Boosting libraries: lambda (L2), alpha (L1), min_child_weight, colsample_bytree, subsample.\n",
        "\n",
        "Random Forest: prefer max_depth/min_samples_leaf rather than fully grown trees.\n",
        "\n",
        "For linear learners: L1/L2 penalties.\n",
        "\n",
        " - Ensemble-specific anti-overfit tactics\n",
        "\n",
        "Bagging / Random Forests: reduces variance — useful when models overfit by high variance. Use OOB score to monitor generalization (and reduce reliance on test set).\n",
        "\n",
        "Boosting: more powerful but easier to overfit — always use early stopping on a temporal validation set and set a conservative learning_rate (e.g., 0.01–0.1).\n",
        "\n",
        "Stacking: avoid overfitting by generating meta-features via out-of-fold predictions only; keep the meta-learner simple (e.g., logistic regression with regularization).\n",
        "\n",
        " - Feature engineering sanity\n",
        "\n",
        "Avoid creating leakage features (e.g., aggregates that include future events relative to prediction time).\n",
        "\n",
        "Reduce high-cardinality noise: group rare categories, target-encode carefully (with smoothing and out-of-fold encoding).\n",
        "\n",
        "Prune features that only improve training but not validation (via permutation importance or validation-based feature selection).\n",
        "\n",
        "  - Handle label imbalance and noise thoughtfully\n",
        "\n",
        "Use class weights or proper loss functions (e.g., focal loss, weighted objective) instead of naive upsampling that can cause overfitting.\n",
        "\n",
        "Inspect label noise — mislabels materially increase overfitting for boosting.\n",
        "\n",
        " - Robust hyperparameter search & nested validation\n",
        "\n",
        "Use randomized or Bayesian search; budget tuning fairly between model families.\n",
        "\n",
        "Prefer nested CV (or at least a holdout backtest) when selecting hyperparameters to avoid optimistic tuning.\n",
        "\n",
        " - Calibration + probability smoothing\n",
        "\n",
        "Overfit models can produce overconfident probabilities. Calibrate on a validation set (Platt / isotonic) before using scores for business decisions or expected-loss computation.\n",
        "\n",
        " - Monitor learning curves & early stopping signals\n",
        "\n",
        "Plot train vs validation metric as training progresses (for boosting — each round; for ensemble sizes — growth curves). Use early stopping when validation stops improving.\n",
        "\n",
        " - Production monitoring & drift detection\n",
        "\n",
        "After deployment, monitor: model performance by cohort/time, feature distributions, and PSI (population stability index). Retrain or roll back when performance degrades.\n",
        "\n",
        "● Select base models\n",
        " - Step-by-step approach to select base models for an ensemble (loan-default)\n",
        "\n",
        "1) Start with the business objective & constraints\n",
        "\n",
        "Decide the primary metric(s): AUC, AUPR, precision@K, recall@K, or expected monetary loss.\n",
        "\n",
        "Note non-functional constraints: inference latency, model size, need for explanations (regulatory), retrain frequency, and compute budget.\n",
        "\n",
        "2) Do a quick data audit\n",
        "\n",
        "Size of dataset, number of features, categorical vs numeric, missingness, class imbalance, and time structure.\n",
        "\n",
        "If data is temporal, plan time-aware splits (train on earlier periods, validate on later).\n",
        "\n",
        "3) Build a candidate pool (accuracy + diversity)\n",
        "\n",
        "Include algorithms that are known to perform well on tabular financial data and some that bring different inductive biases:\n",
        "\n",
        "Logistic Regression (L2/L1) — interpretable baseline, good calibration.\n",
        "\n",
        "Decision Tree (shallow) — interpretable rule behavior.\n",
        "\n",
        "RandomForest / ExtraTrees — bagging, low variance, robust.\n",
        "\n",
        "Gradient Boosters (LightGBM, XGBoost, CatBoost) — often top performers on tabular data.\n",
        "\n",
        "Simple Neural Net (MLP) — useful if data is large / non-linear patterns exist.\n",
        "\n",
        "SVM / kNN / Naive Bayes — only if dataset characteristics make them sensible (small n, special distributions).\n",
        "\n",
        "Rule/scorecard model — keep at least one rule-based or scorecard model if regulators require a simple surrogate.\n",
        "\n",
        "Also treat model variants (same algorithm with different hyperparameters/preprocessing) as distinct candidates to increase ensemble diversity.\n",
        "\n",
        "4) Use identical preprocessing pipelines\n",
        "\n",
        "Create a reproducible feature pipeline (imputation, scaling, encoders, time aggregation) and use it consistently across models.\n",
        "\n",
        "For high-cardinality categoricals, test both one-hot and target encoding (or CatBoost which handles categories natively).\n",
        "\n",
        "5) Evaluate with a time-aware OOF pipeline\n",
        "\n",
        "Use time-based K-fold or walk-forward CV and produce out-of-fold (OOF) predictions for every candidate.\n",
        "\n",
        "Evaluate candidates on the same business metric(s) and on stability across time windows.\n",
        "\n",
        "6) Rank by performance and stability\n",
        "\n",
        "Primary filter: average OOF metric (e.g., AUPR) on validation folds.\n",
        "\n",
        "Secondary filter: variance of that metric across folds (stability over time).\n",
        "\n",
        "Keep models that score high and are stable.\n",
        "\n",
        "7) Measure diversity / complementarity\n",
        "\n",
        "Compute pairwise correlation between OOF predicted probabilities or between error vectors.\n",
        "\n",
        "Low correlation of predictions or complementary errors → better ensemble gains.\n",
        "\n",
        "Useful simple heuristics:\n",
        "\n",
        "Keep models with prediction correlation < ~0.95 (adjust on your data).\n",
        "\n",
        "Prefer an ensemble where top models make different mistakes on hard examples.\n",
        "\n",
        "Formal diversity metrics you can compute from OOFs: disagreement rate, Q-statistic, or simple Pearson correlation of predictions.\n",
        "\n",
        "8) Check calibration & probability quality\n",
        "\n",
        "For risk scoring you often need well-calibrated probabilities (Brier score, calibration curve).\n",
        "\n",
        "Calibrate candidate OOF predictions (Platt / isotonic) if necessary before stacking or blending.\n",
        "\n",
        "9) Filter by production & governance constraints\n",
        "\n",
        "Remove candidates that violate latency, memory, or explainability constraints.\n",
        "\n",
        "If regulation demands, keep at least one simple/surrogate model (e.g., logistic or scorecard) that approximates the ensemble.\n",
        "\n",
        "10) Select final base set (practical rules)\n",
        "\n",
        "For stacking: 3–7 complementary models usually works well — choose a mix of high performers and ones that are diverse.\n",
        "\n",
        "For bagging: use homogeneous weak learners (Decision Trees) but vary seeds/hyperparams/feature subsets.\n",
        "\n",
        "For boosting: base learners are weak trees — hyperparameters control their capacity (not a selection of different algorithms).\n",
        "\n",
        "11) Train final ensemble & perform ablation\n",
        "\n",
        "Use OOF predictions as features to train a simple regularized meta-learner (e.g., logistic regression with L2).\n",
        "\n",
        "Do ablation: remove each base model and measure ensemble performance drop to quantify contribution.\n",
        "\n",
        "12) Robustness, fairness, monitoring\n",
        "\n",
        "Test fairness across protected groups, run stress/backtests (cohort by month), and check sensitivity to feature drift.\n",
        "\n",
        "In production track per-model and ensemble metrics and have rollback/fallback options.\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        " - Step-by-step approach\n",
        "\n",
        "1) Decide business metrics up front\n",
        "\n",
        "Pick one primary metric that maps to business value plus a few secondary diagnostics.\n",
        "\n",
        "Examples: precision@K, recall@K, AUPR (precision-recall), AUROC, and expected monetary loss (cost-sensitive).\n",
        "\n",
        "Also track calibration (Brier score / calibration curve) if you’ll use probabilities for decisions.\n",
        "\n",
        "2) Prepare data to avoid leakage\n",
        "\n",
        "Build modelling rows that represent what you’d have at scoring time (e.g., customer snapshot up to cutoff). Do not include future events in features.\n",
        "\n",
        "If you have multiple records per customer, either aggregate to customer-level snapshots or ensure cross-validation prevents the same customer appearing in train and validation folds.\n",
        "\n",
        "3) Choose the correct split strategy (most important)\n",
        "\n",
        "Pick a CV splitter that respects the data generation process:\n",
        "\n",
        "Temporal / rolling (preferred for loan risk): use walk-forward validation (TimeSeriesSplit or custom rolling windows). Always simulate training on past and validating on future.\n",
        "\n",
        "Group-aware: if multiple rows per customer, use GroupKFold (groups = customer_id) so all rows of a customer stay in one fold.\n",
        "\n",
        "Stratified: for severe class imbalance, use StratifiedKFold (or stratify by label proportion at group level) to keep class ratio similar across folds — but only when temporal leakage is not introduced.\n",
        "\n",
        "Combine constraints: if you need both time and grouping, create time windows and within each window split by groups (or write a custom splitter that yields time windows and enforces group separation).\n",
        "\n",
        "4) Design cross-validation for ensembles / model selection\n",
        "\n",
        "Bagging / RandomForest: OOB score gives a quick internal generalization estimate, but still validate with time/group CV for business metrics.\n",
        "\n",
        "Boosting: use early stopping on a validation partition within each training fold (monitor time-based validation).\n",
        "\n",
        "Stacking / Blending: create out-of-fold (OOF) predictions for each base model using the same CV splitter (preserving time/groups). Train meta-learner on OOFs. Then evaluate final stacked model using an outer (holdout) time fold (nested CV).\n",
        "\n",
        "Nested CV for hyperparameter tuning: outer loop estimates generalization, inner loop tunes hyperparameters — avoid optimistic bias.\n",
        "\n",
        "5) Implement nested/time-aware CV for hyperparameter tuning (practical)\n",
        "\n",
        "Outer loop: rolling/windowed splits for evaluation.\n",
        "Inner loop: for each outer train fold, run hyperparameter search (Grid/Random/Bayesian) using smaller rolling splits inside that train fold. Use early_stopping for boosters to reduce overfit.\n",
        "\n",
        "6) Preserve evaluation parity (same preproc & pipeline)\n",
        "\n",
        "Build a single Pipeline (imputation, encoders, scaling, feature creation) and fit transforms only on training folds. Apply identical pipeline on validation folds. This prevents leakage and ensures fair comparison.\n",
        "\n",
        "7) Handle class imbalance inside CV\n",
        "\n",
        "Use class weights or model objectives that accept weights. If resampling, do it inside each training fold only (never before splitting).\n",
        "\n",
        "Evaluate rare-class metrics (AUPR, precision@K) on validation folds.\n",
        "\n",
        "8) Produce Out-Of-Fold predictions for stacking and diagnostics\n",
        "\n",
        "Save OOF probabilities for each fold and model. Use OOFs to:\n",
        "\n",
        "Train meta-learner for stacking.\n",
        "\n",
        "Compute per-example uncertainty and error patterns.\n",
        "\n",
        "Measure pairwise correlation/diversity between base models.\n",
        "\n",
        "9) Estimate variability & confidence intervals\n",
        "\n",
        "Report mean ± standard deviation across folds for metrics.\n",
        "\n",
        "Compute 95% CI by:\n",
        "\n",
        "percentile bootstrap of fold scores, or\n",
        "\n",
        "repeated CV / repeated rolling windows and reporting percentiles.\n",
        "\n",
        "For time dependence, prefer blocked bootstrap or repeated walk-forward splits rather than naive bootstrap.\n",
        "\n",
        "10) Statistical model comparison (if needed)\n",
        "\n",
        "For paired fold scores, use paired tests (paired t-test if normality plausible, otherwise Wilcoxon signed-rank). For time series, be careful — independence assumption may fail; consider blocked bootstrap or test on multiple non-overlapping holdout periods.\n",
        "\n",
        "11) Calibration, threshold selection & business mapping\n",
        "\n",
        "Calibrate predicted probabilities on validation folds (Platt/isotonic) and evaluate calibrated scores on a final holdout test period.\n",
        "\n",
        "Choose decision thresholds by optimizing the business objective (e.g., maximize expected profit or minimize expected loss) using validation folds — then lock and test on holdout.\n",
        "\n",
        "12) Reporting — make it actionable\n",
        "\n",
        "For each model/family produce:\n",
        "\n",
        "Mean ± std (and 95% CI) of primary metric across outer folds.\n",
        "\n",
        "Per-period performance (show drift or instability across months).\n",
        "\n",
        "Precision/recall at selected K or thresholds, confusion matrix, calibration plot.\n",
        "\n",
        "Expected monetary impact (benefit/cost) at chosen operating point.\n",
        "\n",
        "OOF feature importances / SHAP summaries and stability across folds.\n",
        "\n",
        "13) Production readiness & monitoring plan\n",
        "\n",
        "After choosing model, validate once on a final chronologically later holdout (never used for tuning).\n",
        "\n",
        "Set up monitoring: monthly cohort metrics, PSI, calibration drift, and automated alerts. Plan retraining cadence based on drift.\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        " - Step-by-step justification\n",
        "\n",
        "- Map model improvements to business outcomes first\n",
        "\n",
        "Translate prediction metrics (recall, precision, AUC, calibration) into business KPIs: expected monetary loss, number of prevented defaults, customer churn from false positives, operational cost of manual review.\n",
        "\n",
        "Decide the primary business metric you’ll use to judge models (e.g., expected loss reduction or precision@K).\n",
        "\n",
        "- Run fair experiments (time-aware, group-aware)\n",
        "\n",
        "Train baseline models and ensemble candidates using time-based splits (no leakage).\n",
        "\n",
        "Use the same preprocessing, the same evaluation metric(s) and nested CV for tuning. This ensures any improvement is real and not an artifact.\n",
        "\n",
        "- Quantify metric improvements, not just significance\n",
        "\n",
        "Report absolute metric changes (Δ recall, Δ precision) with confidence intervals across folds and over time windows.\n",
        "\n",
        "Small absolute metric gains can be huge in dollar terms in finance — quantify them.\n",
        "\n",
        "- Convert metric gains to dollar impact\n",
        "\n",
        "Compute expected monetary benefit from improvements (example below). Present conservative and optimistic scenarios (best / typical / worst) to stakeholders.\n",
        "\n",
        "- Show robustness & uncertainty reduction\n",
        "\n",
        "Use ensembles (bagging/stacking/boosting) to reduce variance and show more stable performance across time slices and cohorts.\n",
        "\n",
        "Present reduced variance (std across folds/months) as evidence that decisions will be more predictable and safer.\n",
        "\n",
        "- Demonstrate better probability estimates and decision thresholds\n",
        "\n",
        "Ensembles often yield better-ranked probabilities. Calibrate probabilities (Platt/isotonic) and show improved calibration/Brier score so thresholds map reliably to expected loss.\n",
        "\n",
        "Show how a calibrated ensemble lets you pick thresholds that maximize expected profit or control false positive rate to a target.\n",
        "\n",
        "- Address operational & governance tradeoffs\n",
        "\n",
        "Show that ensemble performance gains justify any extra inference cost, or propose a hybrid (ensemble for scoring offline and a lightweight surrogate for real-time decisions).\n",
        "\n",
        "Provide explainability artifacts (SHAP summaries, feature importances, surrogate rules) so regulators and stakeholders can audit decisions.\n",
        "\n",
        "- Backtest and A/B / shadow deploy\n",
        "\n",
        "Backtest decisions on historic data (simulate decisions and monetary flows).\n",
        "\n",
        "Run a shadow/A-B test comparing current production vs ensemble policy and measure real business lift before full roll-out.\n",
        "\n",
        "- Monitor, measure, and iterate\n",
        "\n",
        "Put monitoring in place (performance by cohort, PSI, calibration drift). Retrain and re-evaluate periodically. Show how ensembles reduce the frequency of urgent interventions via stability."
      ],
      "metadata": {
        "id": "cyn-JNYxzBgr"
      }
    }
  ]
}